{"cells":[{"cell_type":"markdown","source":["# Train using OID for YOLOv7."],"metadata":{"id":"Vwy9IVXAcnH5"}},{"cell_type":"markdown","source":["### By-Pravar Kochar"],"metadata":{"id":"5oRSRQUhcyLt"}},{"cell_type":"markdown","source":["## Get the Data in YOLOv7 labeled format."],"metadata":{"id":"9vyJ4cHAk4aU"}},{"cell_type":"markdown","source":["Download the dataset to train the model on, using a OIDv4 toolkit."],"metadata":{"id":"QfgI38Uodhf5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HcKx3hk1ZAuj"},"outputs":[],"source":["%cd /content\n","!git clone https://github.com/EscVM/OIDv4_ToolKit.git\n","# Install requirements.\n","%cd /content/OIDv4_ToolKit\n","!pip install -r requirements.txt"]},{"cell_type":"code","source":["# Define classes to download (change the classes.txt file)\n","!echo -e 'Man' > /content/OIDv4_ToolKit/classes.txt\n","!echo -e 'Woman' >> /content/OIDv4_ToolKit/classes.txt"],"metadata":{"id":"y5ITJ7B2g9T_","executionInfo":{"status":"ok","timestamp":1692561921799,"user_tz":-240,"elapsed":16,"user":{"displayName":"Pravar Kochar","userId":"14531919094453028296"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Start the download with conditions:\n","*   Using classes as defined in classes.txt\n","*   CSV_type: train\n","*   Allow multiclass\n","*   Photo limit: 500\n","\n"],"metadata":{"id":"zwDygDtRlyZ3"}},{"cell_type":"code","source":["# Download the files.\n","!python main.py downloader -y --classes /content/OIDv4_ToolKit/classes.txt --type_csv train --multiclass 1 --limit 500"],"metadata":{"id":"DbbougBVsAfo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The OID format is different than YOLOv7 format, create the labels txt file using the csv file provided."],"metadata":{"id":"UqBA7Sr-mZVw"}},{"cell_type":"markdown","source":["Pre-process: Reading, filtering, and preparing the data from the CSV file."],"metadata":{"id":"G5qN8pQCm7BZ"}},{"cell_type":"code","source":["# Create yolo txt labels from csv file.\n","import pandas as pd\n","import os\n","\n","classes_data = pd.read_csv('/content/OIDv4_ToolKit/OID/csv_folder/class-descriptions-boxable.csv', header=None)\n","\n","# Get the classes.\n","cl_fl = open('/content/OIDv4_ToolKit/classes.txt', 'r')\n","dt = cl_fl.read()\n","cl_fl.close()\n","\n","classes = dt.split(\"\\n\")[:-1]\n","\n","# Get the Class string ID.\n","class_string = []\n","for i in classes:\n","  req_classes = classes_data.loc[classes_data[1] == i]\n","  string = req_classes.iloc[0][0]\n","  class_string.append(string)\n","\n","# Get columns from the annotation csv file.\n","annotation_data = pd.read_csv('/content/OIDv4_ToolKit/OID/csv_folder/train-annotations-bbox.csv',\n","                              usecols=['ImageID', 'LabelName',\n","                                        'XMin', 'XMax',\n","                                        'YMin', 'YMax'])\n","\n","# Filter the classes.\n","filtered_class_data = annotation_data.loc[annotation_data['LabelName'].isin(class_string)].copy()\n","\n","# Add columns for for YOLO format.\n","filtered_class_data['classNumber'] = ''\n","filtered_class_data['center x'] = ''\n","filtered_class_data['center y'] = ''\n","filtered_class_data['width'] = ''\n","filtered_class_data['height'] = ''\n","\n","# Assign a class\n","for i in range(len(class_string)):\n","  filtered_class_data.loc[filtered_class_data['LabelName'] == class_string[i], 'classNumber'] = i\n","\n","# Calc x-center, y-center, width, height.\n","filtered_class_data['center x'] = (filtered_class_data['XMax'] + filtered_class_data['XMin']) / 2\n","filtered_class_data['center y'] = (filtered_class_data['YMax'] + filtered_class_data['YMin']) / 2\n","filtered_class_data['width'] = filtered_class_data['XMax'] - filtered_class_data['XMin']\n","filtered_class_data['height'] = filtered_class_data['YMax'] - filtered_class_data['YMin']\n","\n","YOLO_values = filtered_class_data.loc[:, ['ImageID', 'classNumber', 'center x', 'center y', 'width', 'height']].copy()"],"metadata":{"id":"4VksU1nPAZJT","executionInfo":{"status":"ok","timestamp":1692562034856,"user_tz":-240,"elapsed":1025,"user":{"displayName":"Pravar Kochar","userId":"14531919094453028296"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Create the YOLO format label files."],"metadata":{"id":"uf_x1-iAm1Ys"}},{"cell_type":"code","source":["# Change current dir to images.\n","img_path = '/content/OIDv4_ToolKit/OID/Dataset/train/Man_Woman'\n","os.chdir(img_path)\n","\n","# loop through all files in dir.\n","for curr_dir, dirs, files in os.walk('.'):\n","  for f in files:\n","    if f.endswith('.jpg'):\n","      img_title = f[:-4]  # Get name of img.\n","      YOLO_file = YOLO_values.loc[YOLO_values['ImageID'] == img_title]\n","\n","      # Create copy.\n","      df = YOLO_file.loc[:, ['classNumber', 'center x', 'center y', 'width', 'height']].copy()\n","\n","      # Path to save on.\n","      save_path = img_path + '/' + img_title + '.txt'\n","\n","      # Generate file.\n","      df.to_csv(save_path, header=False, index=False, sep=' ')\n","\n","# Remove the old labels to clear up space.\n","!rm -r /content/OIDv4_ToolKit/OID/Dataset/train/Man_Woman/Label"],"metadata":{"id":"eHTW-OYbe14Y","executionInfo":{"status":"ok","timestamp":1692562057945,"user_tz":-240,"elapsed":2445,"user":{"displayName":"Pravar Kochar","userId":"14531919094453028296"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Verification to check if labels are successfully created.\n","!echo 'All files: ' | ls -l /content/OIDv4_ToolKit/OID/Dataset/train/Man_Woman/*.* | grep -v ^l | wc -l\n","!echo 'JPG files: ' | ls -l /content/OIDv4_ToolKit/OID/Dataset/train/Man_Woman/*.jpg | grep -v ^l | wc -l\n","!echo 'TXT files: ' | ls -l /content/OIDv4_ToolKit/OID/Dataset/train/Man_Woman/*.txt | grep -v ^l | wc -l"],"metadata":{"id":"0X8TWQvMn2dG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After correct labels are created, split data to train, val, test data. (Move the files using split-folders library)"],"metadata":{"id":"rSk9fXIbBEko"}},{"cell_type":"code","source":["# To split data in train and test.\n","!pip install split-folders"],"metadata":{"id":"MJYaflPgCclz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split data.\n","!splitfolders --output /content/gender_data --move --group_prefix 2 --ratio .8 .1 .1 -- /content/OIDv4_ToolKit/OID/Dataset/train"],"metadata":{"id":"T-RkFNIf4btn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Split the JPG and TXT files to be in images and labels under the respective type of data."],"metadata":{"id":"nH8yp_spoZB0"}},{"cell_type":"code","source":["# train folder correction.\n","!cd /content/gender_data/train\n","!mkdir /content/gender_data/train/images\n","!mkdir /content/gender_data/train/labels\n","!mv /content/gender_data/train/Man_Woman/*.jpg /content/gender_data/train/images\n","!mv /content/gender_data/train/Man_Woman/*.txt /content/gender_data/train/labels\n","!rm -r /content/gender_data/train/Man_Woman\n","\n","# test folder correction.\n","!cd /content/gender_data/test\n","!mkdir /content/gender_data/test/images\n","!mkdir /content/gender_data/test/labels\n","!mv /content/gender_data/test/Man_Woman/*.jpg /content/gender_data/test/images\n","!mv /content/gender_data/test/Man_Woman/*.txt /content/gender_data/test/labels\n","!rm -r /content/gender_data/test/Man_Woman\n","\n","# validation folder correction.\n","!mv /content/gender_data/val /content/gender_data/valid\n","!cd /content/gender_data/valid\n","!mkdir /content/gender_data/valid/images\n","!mkdir /content/gender_data/valid/labels\n","!mv /content/gender_data/valid/Man_Woman/*.jpg /content/gender_data/valid/images\n","!mv /content/gender_data/valid/Man_Woman/*.txt /content/gender_data/valid/labels\n","!rm -r /content/gender_data/valid/Man_Woman"],"metadata":{"id":"9XzmIhiWInmq","executionInfo":{"status":"ok","timestamp":1692562060948,"user_tz":-240,"elapsed":2069,"user":{"displayName":"Pravar Kochar","userId":"14531919094453028296"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["Generate the YAML file to direct the YOLOv7 trining file."],"metadata":{"id":"_9x7h8buok_Y"}},{"cell_type":"code","source":["# Make yaml file.\n","!cd /content/gender_data\n","!touch /content/gender_data/gender_yolo.yaml\n","!echo -e 'names:' > /content/gender_data/gender_yolo.yaml\n","!echo -e '- Man' >> /content/gender_data/gender_yolo.yaml\n","!echo -e '- Woman' >> /content/gender_data/gender_yolo.yaml\n","!echo -e 'nc: 2' >> /content/gender_data/gender_yolo.yaml\n","!echo -e 'train: /content/gender_data/train/images' >> /content/gender_data/gender_yolo.yaml\n","!echo -e 'val: /content/gender_data/valid/images' >> /content/gender_data/gender_yolo.yaml\n","!echo -e 'test: /content/gender_data/test/images' >> /content/gender_data/gender_yolo.yaml"],"metadata":{"id":"dHK3ctJyznHL","executionInfo":{"status":"ok","timestamp":1692562061473,"user_tz":-240,"elapsed":531,"user":{"displayName":"Pravar Kochar","userId":"14531919094453028296"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["## Get the YOLOv7 repository preped up for training on OID dataset."],"metadata":{"id":"DJgh1YcxovWK"}},{"cell_type":"markdown","source":["Clone the YOLOv7 repo to host and install the requirements for the model."],"metadata":{"id":"FQR91y5Nc3tB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"syFRWLQZdnLi"},"outputs":[],"source":["!git clone https://github.com/WongKinYiu/yolov7.git\n","%cd yolov7\n","!pip install -r requirements.txt"]},{"cell_type":"markdown","source":["In the cloned repository, download the weigths file of desire to run the respective YOLOv7 model. (Here the custom trained YOLOv7 is chosen as the model to run)"],"metadata":{"id":"pwX-VqYIdOd5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"sFkpgmIof3qo"},"outputs":[],"source":["%cd /content/yolov7/\n","!wget \"https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-tiny.pt\""]},{"cell_type":"markdown","source":["Train the model with the given dataset with the following parameters.\n","*   Batch size: 82\n","*   Epochs: 200\n","*   CFG: /content/yolov7/cfg/training/yolov7-tiny.yaml\n","*   Using the downloaded weights, data, and the train.py."],"metadata":{"id":"zq2xkzLUM_pi"}},{"cell_type":"code","source":["%cd /content/yolov7/\n","!python train.py --batch 82 --cfg /content/yolov7/cfg/training/yolov7-tiny.yaml --epochs 200 --data /content/gender_data/gender_yolo.yaml --weights /content/yolov7/yolov7-tiny.pt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NEMbfJEvYHZE","executionInfo":{"status":"ok","timestamp":1692562095465,"user_tz":-240,"elapsed":33999,"user":{"displayName":"Pravar Kochar","userId":"14531919094453028296"}},"outputId":"777b2834-8a38-4850-e52e-de9c05048c9b"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/yolov7\n","2023-08-20 20:07:45.609192: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-08-20 20:07:46.522617: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","YOLOR 🚀 v0.1-126-g84932d7 torch 2.0.1+cu118 CUDA:0 (Tesla T4, 15101.8125MB)\n","\n","Namespace(weights='/content/yolov7/yolov7-tiny.pt', cfg='/content/yolov7/cfg/training/yolov7-tiny.yaml', data='/content/gender_data/gender_yolo.yaml', hyp='data/hyp.scratch.p5.yaml', epochs=2, batch_size=64, img_size=[640, 640], rect=False, resume=False, nosave=False, notest=False, noautoanchor=False, evolve=False, bucket='', cache_images=False, image_weights=False, device='', multi_scale=False, single_cls=False, adam=False, sync_bn=False, local_rank=-1, workers=8, project='runs/train', entity=None, name='exp', exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias='latest', freeze=[0], v5_metric=False, world_size=1, global_rank=-1, save_dir='runs/train/exp', total_batch_size=64)\n","\u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.2, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.0, paste_in=0.15, loss_ota=1\n","\u001b[34m\u001b[1mwandb: \u001b[0mInstall Weights & Biases for YOLOR logging with 'pip install wandb' (recommended)\n","Overriding model.yaml nc=80 with nc=2\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1       928  models.common.Conv                      [3, 32, 3, 2, None, 1, LeakyReLU(negative_slope=0.1)]\n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2, None, 1, LeakyReLU(negative_slope=0.1)]\n","  2                -1  1      2112  models.common.Conv                      [64, 32, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n","  3                -2  1      2112  models.common.Conv                      [64, 32, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n","  4                -1  1      9280  models.common.Conv                      [32, 32, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n","  5                -1  1      9280  models.common.Conv                      [32, 32, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n","  6  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n","  7                -1  1      8320  models.common.Conv                      [128, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n","  8                -1  1         0  models.common.MP                        []                            \n","  9                -1  1      4224  models.common.Conv                      [64, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 10                -2  1      4224  models.common.Conv                      [64, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 11                -1  1     36992  models.common.Conv                      [64, 64, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 12                -1  1     36992  models.common.Conv                      [64, 64, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 13  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 15                -1  1         0  models.common.MP                        []                            \n"," 16                -1  1     16640  models.common.Conv                      [128, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 17                -2  1     16640  models.common.Conv                      [128, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 19                -1  1    147712  models.common.Conv                      [128, 128, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 20  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n"," 21                -1  1    131584  models.common.Conv                      [512, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 22                -1  1         0  models.common.MP                        []                            \n"," 23                -1  1     66048  models.common.Conv                      [256, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 24                -2  1     66048  models.common.Conv                      [256, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 25                -1  1    590336  models.common.Conv                      [256, 256, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 26                -1  1    590336  models.common.Conv                      [256, 256, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 27  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n"," 28                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 29                -1  1    131584  models.common.Conv                      [512, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 30                -2  1    131584  models.common.Conv                      [512, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 31                -1  1         0  models.common.SP                        [5]                           \n"," 32                -2  1         0  models.common.SP                        [9]                           \n"," 33                -3  1         0  models.common.SP                        [13]                          \n"," 34  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n"," 35                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 36          [-1, -7]  1         0  models.common.Concat                    [1]                           \n"," 37                -1  1    131584  models.common.Conv                      [512, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 38                -1  1     33024  models.common.Conv                      [256, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 39                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 40                21  1     33024  models.common.Conv                      [256, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 41          [-1, -2]  1         0  models.common.Concat                    [1]                           \n"," 42                -1  1     16512  models.common.Conv                      [256, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 43                -2  1     16512  models.common.Conv                      [256, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 44                -1  1     36992  models.common.Conv                      [64, 64, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 45                -1  1     36992  models.common.Conv                      [64, 64, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 46  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n"," 47                -1  1     33024  models.common.Conv                      [256, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 48                -1  1      8320  models.common.Conv                      [128, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 49                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 50                14  1      8320  models.common.Conv                      [128, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 51          [-1, -2]  1         0  models.common.Concat                    [1]                           \n"," 52                -1  1      4160  models.common.Conv                      [128, 32, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 53                -2  1      4160  models.common.Conv                      [128, 32, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 54                -1  1      9280  models.common.Conv                      [32, 32, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 55                -1  1      9280  models.common.Conv                      [32, 32, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 56  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n"," 57                -1  1      8320  models.common.Conv                      [128, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 58                -1  1     73984  models.common.Conv                      [64, 128, 3, 2, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 59          [-1, 47]  1         0  models.common.Concat                    [1]                           \n"," 60                -1  1     16512  models.common.Conv                      [256, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 61                -2  1     16512  models.common.Conv                      [256, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 62                -1  1     36992  models.common.Conv                      [64, 64, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 63                -1  1     36992  models.common.Conv                      [64, 64, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 64  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n"," 65                -1  1     33024  models.common.Conv                      [256, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 66                -1  1    295424  models.common.Conv                      [128, 256, 3, 2, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 67          [-1, 37]  1         0  models.common.Concat                    [1]                           \n"," 68                -1  1     65792  models.common.Conv                      [512, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 69                -2  1     65792  models.common.Conv                      [512, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 70                -1  1    147712  models.common.Conv                      [128, 128, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 71                -1  1    147712  models.common.Conv                      [128, 128, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 72  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n"," 73                -1  1    131584  models.common.Conv                      [512, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 74                57  1     73984  models.common.Conv                      [64, 128, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 75                65  1    295424  models.common.Conv                      [128, 256, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 76                73  1   1180672  models.common.Conv                      [256, 512, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"," 77      [74, 75, 76]  1     19838  models.yolo.IDetect                     [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n","/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","Model Summary: 263 layers, 6017694 parameters, 6017694 gradients, 13.2 GFLOPS\n","\n","Transferred 330/344 items from /content/yolov7/yolov7-tiny.pt\n","Scaled weight_decay = 0.0005\n","Optimizer groups: 58 .bias, 58 conv.weight, 61 other\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/gender_data/train/labels' images and labels... 15 found, 0 missing, 0 empty, 0 corrupted: 100% 15/15 [00:00<00:00, 1257.96it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/gender_data/train/labels.cache\n","\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/gender_data/valid/labels' images and labels... 1 found, 0 missing, 0 empty, 0 corrupted: 100% 1/1 [00:00<00:00, 552.25it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/gender_data/valid/labels.cache\n","\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 4.78, Best Possible Recall (BPR) = 1.0000\n","Image sizes 640 train, 640 test\n","Using 2 dataloader workers\n","Logging results to runs/train/exp\n","Starting training for 2 epochs...\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","  0% 0/1 [00:13<?, ?it/s]\n","Traceback (most recent call last):\n","  File \"/content/yolov7/train.py\", line 616, in <module>\n","    train(hyp, opt, device, tb_writer)\n","  File \"/content/yolov7/train.py\", line 363, in train\n","    loss, loss_items = compute_loss_ota(pred, targets.to(device), imgs)  # loss scaled by batch_size\n","  File \"/content/yolov7/utils/loss.py\", line 585, in __call__\n","    bs, as_, gjs, gis, targets, anchors = self.build_targets(p, targets, imgs)\n","  File \"/content/yolov7/utils/loss.py\", line 711, in build_targets\n","    pair_wise_iou = box_iou(txyxy, pxyxys)\n","  File \"/content/yolov7/utils/general.py\", line 465, in box_iou\n","    inter = (torch.min(box1[:, None, 2:], box2[:, 2:]) - torch.max(box1[:, None, :2], box2[:, :2])).clamp(0).prod(2)\n","KeyboardInterrupt\n","^C\n"]}]},{"cell_type":"markdown","source":["## After successfull training, test the best weights."],"metadata":{"id":"UukKUJ1Vr8Pk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"563gF8lS0JPh"},"outputs":[],"source":["# Testing the model (Check the exp# to be the correct one).\n","!python test.py --data /content/gender_data/gender_yolo.yaml --img 640 --batch 64 --conf 0.01 --weights /content/yolov7/runs/train/exp/weights/best.pt --name OID_yolo_test"]},{"cell_type":"markdown","source":["Save the best.pt and every other relevent file to drive."],"metadata":{"id":"k1jWiuMAt-OI"}},{"cell_type":"code","source":["!cp /content/yolov7/runs/test/OID_yolo_test /content/drive/"],"metadata":{"id":"BLOz3VdMt7Zo"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Km6EfAK50qbI"},"outputs":[],"source":["# If want to run detection on random 10 samples.\n","%cd /content/yolov7\n","!python detect.py --weights /content/yolov7/custom_face_yolo.pt --conf 0.4 --source /content/yolov7/data/images\n","\n","import glob\n","from IPython.display import Image, display\n","\n","i = 0\n","limit = 10\n","# Check if the detect run to be printed is exp or exp1/2/...\n","for imageName in glob.glob('/content/yolov7/runs/detect/exp/*.jpg'):\n","  if i < limit:\n","    display(Image(filename=imageName))\n","    print(\"-\"*15)\n","  i = i + 1"]}],"metadata":{"colab":{"provenance":[{"file_id":"1K8FgABNS9fLadOlzuUwPYP6QFZQK_BjK","timestamp":1691802270053},{"file_id":"1mHubtEt9ypFT3HLYURwXf3nNICV6Zcso","timestamp":1691780740201}],"gpuType":"T4","mount_file_id":"1P6fCBvnP7-pgzvJb3Y-ie2wtllbrD6Yq","authorship_tag":"ABX9TyOfGeT+l6eb08aar8hKIYnB"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}